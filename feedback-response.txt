-------- Reviewer #1

[x] ...the authors do not address linkages within cited datasets. For instance, media within Darwin Core Archives are often represented as URL links to images, audio and/or other media. These internal links would face the same problems of "link rot" as the cited parent container.
> Added to future work section: "In this study, we only monitored the URLs that locate datasets. However, datasets may internally contain references to other data, such as media, literature, and genetic sequence information \citep{Wieczorek_2012}. Such references are often URLs and therefore potentially unreliable. For datasets that contain links to other data, a recursive approach could be considered where those links are themselves queried for content and tracked through provenance records. This is the subject of future work and beyond the scope of this paper."

[x] The current mechanism of bundling records as independent datasets for publication may not scale when citing datasets with large numbers of images, CT scans, audio recordings etc. ... Finally, the work presented also does not discuss any potential impacts of performance when calculating hashes for large datasets (CT scans for example) on the scale of 100s of GB.
> No change -- since only references to those images, CT scans, etc. are included in biodiversity datasets, those resources are not being hashed or versioned. As mentioned in the above point, addressing linkages within cited datasets is listed as future work

[x] include a note to discussing the implications (or future work needed) for preserving internal links and large digital assets, which are becoming more prevalent within biodiversity datasets
> Added (see first point)

-------- Reviewer #2

Introduction & Problem characterization sections:
[x] These two sections include several passages that refer to the results/conclusions. This makes it a bit repetitive.
> Rephrased and dropped some sentences in Problem Characterization

[x] Typo in Introduction 1st paragraph: “…and periodic archiving (Costello et al. 2013), , no systematic approach...”, two commas after Costello et al. citation.
> Fixed!


Methodology
URL Reliability
[x] ...clarification of why you consider this time frame as long term, and how you would expect the patterns to change if other time frames were taken.
> Clarified in future work section: "The seven-month span of our experimental results might not be considered long-term in the context of biodiversity data networks. To evaluate the long-term reliability of URLs in the selected data networks, continued monitoring is needed."

[x] In the sentence: “Monitoring URLs in this way allows us not only to determine whether link rot and content drift occur, but also to capture their long-term behaviors”, you may consider referencing the Data Collection Over Time section, for clarity.
> Added reference at the end of the section

[x] “We characterize the responsiveness of a URL according to how often it exhibits link rot: unresponsive … responsive...”. I believe this would not really be how often, but rather whether it exhibits the behaviour or not.
> Changed "how often" to "whether"

[x] “Before we can determine the reliability of any given URL, we must first monitor its behavior over time by documenting how it responds to periodic queries.” You may consider referencing the Data Collection Over Time section.
> Added reference: "In order to determine the reliability of any given URL over time, we must monitor its behavior by documenting how it responds to periodic queries. We propose a method for monitoring URL behavior in the Data Collection Over Time section of this paper."

The Data Collection Process
[x] “This can be achieved by deriving the identifiers from the content of their respective datasets”. Although it becomes quite clear later on, it may ease reading to provide a very short example here of what this would be.
> Reorganized text and added example: "For example, given two different bits of text, "first example" and "second example", their computed SHA-256 hashes (in hexadecimal format) are b84283f1f4cb997eaeb28dce84466678ea611824ac97978749b158d2cd3886ac and c64eee387ccc1d0438765129a8c423dab0b67d094710e395ac3193c52591a3ba, respectively."

[x] “With these restrictions in place, it is possible to reliably link each recorded dataset to its provenance record without the need for an intermediate index.”. This sentence may be a little confusing.
> Reworded and expanded to "the derivation of the content-based identifier for a given dataset can be performed by anyone, anywhere, and at any time. There is no need for some central authority to generate and assign identifiers, as is the case for non-content-based identification schemes \citep{Paskin_1999}. Therefore, dataset provenance can be collected in a decentralized manner; if two agents collect provenance for the same dataset acquired from potentially different locations, they can both reference the dataset using the same content-based identifier without any need for coordination. In this scenario, the two provenance records produced by the two agents can each be uniquely identified by using content-based identifiers. We elaborate on uses for identifying and referencing provenance records in the discussion section of this paper."

[ ] a brief explanation of why this choice (SHA-256) over others, very briefly which are the advantages of the one you chose.
> No change -- we provide some reasoning at the end of Transitioning to Reliable References

Data Collection Over Time
[x] first paragraph: “By establishing a dedicated data observatory that implements the collection process we have described...”. The description of the data observatory is actually done after this sentence, particularly in the Experiment section, but not above. I’d suggest referring to that section here.
> Removed "that implements the collection process we have described"

[x] first paragraph. I’d probably include somewhere in this paragraph a reference to where the responses and the provenance datasets can be found.
> No change -- at this point, we're only talking about methodology; access to datasets and provenance is up to the implementation, and we give some examples later on (e.g. the hash-archive.org figure)

Data Network Reliability
[x] “First, we label individual URLs with binary indicators of responsiveness, stability, and reliability.”. I’d suggest to add something like “…at any given time they were queried”.
> Added "at each time they were queried"

Results
[x] second paragraph: it is not entirely clear how the 5% and 4% are calculated, as you are assessing over time; please clarify.
> No change -- we state that we are talking specifically about the change from one query to the next for the same URL
> Reworded "the next time it responded when queried" to "in response to the next successful query"

[x] table 1: Clarify in the caption that numbers in brackets are the total URL count (it’s in the text, but much easier to have it in the caption when looking at the table).
> Added "Numbers in brackets indicate total URL counts."

Sources of Potential Numerical Error
[x] “Additionally, we only queried URLs that the data networks list in their dataset registries; this means that, if a URL were removed from a network’s registry, would not be able to detect subsequent instances of reference rot.” Having the logs, I guess you could look and see how many URLs were in the registry at any given month but disappeared the next month, to have an estimate of this bound?
> Right! URLs removed from networks can be detected by inspecting the provenance logs. No change made -- although the observation is correct, it's not necessary for explaining the results or drawing conclusions

[x] last sentence: “In an effort to minimize…”. This probably belongs in the Methods section, and not in the Results.
> Moved sentence to Experiment section of Methods

Discussion
[x] how would unresponsiveness be fully overcome. Whatever the data store, and as improbable as may be, it would also be susceptible to this problem.
> No change -- we actually already address this in "An Alternative: Unique Content-Based Identifiers": "When several repositories serve referenced datasets, there is no single point of failure for content hash lookups; if a referenced dataset is redundantly located across and within data repositories, access to the dataset will only be lost if all associated locations exhibit link rot. even if access to a dataset is lost, it can be restored as long as the referenced dataset still exists somewhere and can be made discoverable and accessible."

[x] practically speaking, who would assign the hashes? If replicating state of things now, that would probably fall on the aggregator’s table. If you think that would be the way to go, it may be worth mentioning that explicitly.
> No change -- since the hashes are deterministically derived using SHA-256, no one person/software/organization needs to assign them

[x] although a very very small chance, there could be hash collisions. Who should take care of dealing with that situation if it happened? And, what would be the protocol to follow in such cases? (e.g., which would be the second best algorithm choice).
> No change -- the chance of collision is effectively none; to our knowledge, no collision for SHA-256 has ever been found

[x] the amount of data to be stored, though not comparable with other huge datasets from other realms –eg twitter-, would not be trivial. Would there be costs to keeping all those data in store ad infinitum? Who would take on those costs? ...should we think of a centralized data store for all the community, or would we expect each data network to go separate ways?
> No change -- central to our ideas is that you can augment existing data networks to allow for decentralized registries and storage. In our minds, this architecture better reflects the social and financial boundaries imposed by institutions, disciplines etc.

[x] should there be a consensus on which information to include in the provenance?
> No change -- we suggest some minimum requirements in "Enhancing Dataset References with Provenance": "The types of information in the provenance depend on the implementation of the data observatory, but at a minimum include the URLs that were queried to produce the content, the dates of the queries, the format of the content, and the data registries that were searched to find the content."

References
[ ] Matsunaga et al. reference may be incomplete.
> It looks fine to me, maybe there was an error when converting our paper to Word?

-------- General changes
> Figure 2 : changed title from "Network reliabilities over time" to "Network URL characteristics over time"

> Changed "URLs that locate datasets are analogous to the physical locations of specimens in the natural world" to "URLs that locate datasets on the internet are analogous to the physical locations of specimens in the natural world"

> Page 12: changed "Integrated Digitized Bio Collections" to "Integrated Digitized Biocollections"

> Page 19: changed "using URLs to identify evolving datasets rather than fixed dataset versions" to "using URLs to identify evolving datasets rather than using content-based identifiers to identify fixed dataset versions"

> Changed "A fifth observatory was constructed by aggregating the queries of the four data network observatories" to "To analyze the full set of URLs observed across all four networks, a fifth observatory was constructed by aggregating the provenance records produced by the four data network observatories"

> Reworded much of results section to reflect that we are characterizing the reliabilities of the URLs observed in the data networks, not the data networks themselves

> Added clarification to Experiment section about the kinds of URLs we tracked: "It is important to note that the URLs discovered in data network registries are the sources of the datasets in each network, not necessarily the datasets served by the data networks, which may have been altered to, for example, achieve consistent taxonomy"

> Added reference to GBIF Backbone Taxonomy

> In Unreliability of Location-Based Identifiers section: added explanation that instability in URLs is not inherently bad: "The instability that we have observed across the URLs in the data networks is actually to be expected, and does not in any way evaluate the quality of the data networks. In fact, regular updates to datasets (i.e., URL instability) might indicate continued growth, maintenance, and refinement of those datasets. One might even argue that a stable dataset URL would indicate that the dataset is no longer being maintained or is potentially outdated. Therefore, the issues resulting from the use of URLs as references are not due to poor management on the part of data aggregators or curators, but rather due to the fact that URLs are inherently unreliable."

> Added additional use case for provenance: "Provenance can also be used for attribution purposes; a detailed record is kept of the life of each dataset, including when and where it was found, as well as snapshots of data network registries, which may provide information such as the publisher, authors, and contact information for each dataset."